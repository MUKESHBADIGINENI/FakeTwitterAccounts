\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{fontenc}
\usepackage{listings}
\usepackage{multicol}
\usepackage{verbatim}

\title{UNamur\\
	ICYBM201 Big Data and Computer Security : Fame for sale, efficient detection of fake Twitter
followers}

\author{TIO NOGUERAS Gérard, NYAKI Loïc}

\begin{document}
\maketitle

\newpage
\tableofcontents
\newpage

\section{Introduction}

\paragraph{}
The objective of this project is to partially reproduce the results presented in a 2015 paper titled  \textit{Fame for sale: effective detection of fake twitter followers}, by Cresci \textit{et al.}\\

In the paper, various classification rules and features proposed by academics, \textit{technology bloggers} and companies specialized in fake twitter users detection are enumerated and assessed. Rule-based classification is shown to perform more poorly than feature-based classification. Therefore the authors decided to abandon rule-based classification in favor of the 19 features responsible for the most information gain while having the smallest calculation cost.\\

The resulting classifier trained on that 19-features set shows an accuracy of up to 97.5\% on randomly sampled account, while being solely based on data readily available on the profile of the twitter users, which ensures that the data processing is fast and lightweight.

\subsection{Objectives}
In this project, we focus on reproducing the final results of Cresci \textit{et al.}, by implementing various classifiers based on the feature set presented in that paper.\\

First we describe the 19 features that will be part of the feature set. Then, we introduce the datasets that have been provided, and which contain \textit{human} or \textit{bot} data. Later we describe how the features will be extracted from these datasets as well as our planned procedure for training classifier. Each type of classifier is then described along with their parameters, and results are presented for each classifier. Finally, we summarize the results argue on the difference between the results presented in Cresci \textit{et al.}, and the results we managed to obtain.


\section{Rule sets and Features set}
In the paper, five rule sets and features set are analyzed. They come from  academic research, as well as technology bloggers and companies specialized in fake tweeter users detection.\\

To avoid simply using all the features mentionned in other papers without any preprocessing, the authors decided to measure each feature individually and asses it's effectiveness thanks to machine learning metrics. After doing, they removed the features they considered not to have a sufficient score and removed the features they considered impossible to obtain. (ie. time related features that would involve monitoring of thousands of accounts.)
\\\\
We have decided that we will present the features that they kept and that we will se to train our classifiers.\\
We have devided the features by paper and in each section we have separated the features related to the profile(class A), the timeline(class B) and relationships (class C).

\subsection{Camisani-Calzolari rule set}
In this paper the features had as objective to focus on human accounts, but they can still be used to detect bots. \\
We start with the class A section, for example a couple of straightforward ones: has name, has image, has address, has biography, belongs to a list.
Pretty simple most human users will fill this type of information.\\
The next ones assess that the account has been active and tries to find attributes that will correlate with humans: followers $\geq$ 30, tweets $\geq$ 50, URL
in profile, 2 $*$ followers $\geq$ friends.\\
For the class B of this paper where we try to find information in the tweets of the user that can be relatable to humans. Some patterns that have been noticed that humans follow: geo-localized, is favorite, uses punctuation, uses hashtag. Another pattern that they noticed is that humans don't use the twitter api that much but rather a lot of different items available to us: uses iPhone, uses Android, uses Foursquare, uses Instagram, uses Twitter.com and using multiple clients. This last ones are some additional ones found to be done by humans: userID in tweet,
tweets with URLs and retweet $\geq$ 1. 

The rule set contained the following classes of rules:

\begin{itemize}
\item Camisani-Calzolari: The Camisani-Calzolari rule set is described as follows: 22 class A rules
\item State of search:They propose 7 rules, with 5 of class A, and 2 of class B
\item SocialBakers: The rule set of social baker is composed of 9 rules, with 6 of class A and 3 of class B.
\item Stringhini et al.: This paper proposes 5 features with 3 class A features, and 2 class B features
\item Yang et al.:This paper proposes 9 features. Two of class A, 3 of class B, and 4 of class C.
\end{itemize}

\subsection{State of search}
This ones are really general ones for bot detection by state of search website.
For the class A: bot in biography, friends/followers == 100 and duplicate profile pictures.\\
For class B: same sentence to many accounts, tweet from API.
This features are actually extracted from spammer bots.

\subsection{Socialbakers}
We start finding a trend with some of the ratios for our class A: friends/followers $\geq$ 50, friends $\geq$ 100. Followed by some features resembling the ones in Camisani: default image after 2 months, no bio, no location, 0 tweets.\\
For the Class B, we're only looking at ratios and tweets' similarities: tweets spam phrases, same tweet$\geq$ 3, retweets $\geq$ 90\%, tweet-links $\geq$ 90\%.
\subsection{Stringhini et al.}
The last 2 papers already have really strong features that we are going to use and see if we can improve the score they obtained by adding extra features.
Here you can see some of the best ones: number of friends, number of tweets, $friends/(followers^2)$.\\
For class B: we only have a couple of ones: tweet similarity, URL ratio.
This paper also tracked spam bots.

\begin{tabular}{lc}
\hline
1. spambots do not have thousands of friends; & spambots have a high ratio of tweets containing urls\\
2. spambots have sent less than 20 tweets; & spambots have a high ratio between the total number of tweets from friends and the square of their total followers (lower ratio means legitimate user)\\
3. the content of the spambots' tweets exhibits "message similarity"; & \\
\hline
\end{tabular}


\subsection{Yang et al.}
This last paper has a focus on the API for class B features: API ratio, API URL ratio, API tweet similarity. And some curious features for the class A: age, following rate. The following rate is obvious, not much people follow bots and the age is smarter, spam bots are susceptible to be banned or simply used once therefore their age will usually be lower then real accounts.

\subsection{Rules that were not implemented}
The following features where not implemented,as knowing whether a tweet came from an API was far from obvious : \\

\begin{itemize}
\item{get\_api\_url\_ratio(): returns the ratio between the number of tweets containing a URL and the number of tweets sent from an API.}
\item{get\_api\_tweet\_similarity(): supposedly returns a value representing the similarity between tweets sent from an API.}
\end{itemize} 

We tried using the \textit{grep} command with the keyword "API" on the tweets.csv files but only detected a few results who seemed unrelated.

\section{Data extraction}
To generate the final feature set, we extracted the class A features (data easily obtained through the user profile) as well as the class C features (all the features, ranging from the easiest to obtain to the hardest one, requiring many computations).\\

The result was a features dataset that is ready to be used for training classifiers.

\subsection{Available data}
The researchers responsible for the paper Fame forl sale made their 5 basic datasets available for future researchers.\\ 

Since we are trying to replicate the work they have done to practice our skills and understand more deeply the meaning of data and the proper way to analyse data is by following their BAS dataset creation.

\subsection{Table 1 creation}
In this section we are going to create the base dataset that we will use throughout the project.
We have 5 available datasets: 
We are going to create 1 final dataset. The BAS dataset constituted of 1950 human twitter accounts and 1950 fake accounts.\\ The human accounts are simply the sum of the human datasets we had available.\\
For the fake accounts, we randomly undersampled the 3 datasets available to obtain the same number of accounts as the normal ones.\\
After undersampling the users, we used the ids of these users to collect the rest of the data in the other files.\\\\
Small differences due to the randomness of the users choice:
We encountered a significant drop first with the number of tweets averaging 92k followers which far from the 118k from the paper. We later realized that our parser had and issue with the tweets containing commas and was missing them because of that error.\\

\begin{tabular}{lcccccc}
dataset & accounts & tweets & followers & friends & total relationships \\
\hline
TFP & 469 & 563,693 & 258,494 & 241,710 & 500,204 \\
E13 & 1481 & 2,068,037 & 1,526,944 & 667,225 & 2,194,169 \\
FSF & 1169 & 22,910 & 11,893 & 253,026 & 264,919 \\
INT & 1337 & 58925 & 23173 & 517485 & 540658 \\
TWT & 845 & 114192 & 28588 & 729839 & 758427 \\
\hline
HUM & 1950 & 2,631,730 & 1,785,438 & 908,935 & 2,694,373 \\
FAK & 1950 & 107,031 & 35,404 & 873,494 & 908,898 \\
\hline
BAS & 3900 & 2,738,761 & 1,820,842 & 1,782,429 & 3,603,271 \\
\end{tabular}


\section{Data Pre-Processing}
\paragraph{}
The datasets that we received was composed of several directories containing each the following files: \textit{users.csv, friends.csv, followers.csv and tweets.csv}. These are regular csv files containing text, numerical values and NaN values.\\

NaN values are bothersome as they are there own type and can cause problems when they get mixed with Strings or numerals. Therefore the first thing to do was to replace every NaN instance by something less troublesome. We decided that an empty String would be a good solution. This was done in a single command, when opening the CSV file:

\begin{lstlisting}
pd.read_csv(totalPath, encoding='latin-1').fillna('')
\end{lstlisting}

\section{Feature set generation}
Based on \textit{Cresci et al.}, 3 classes of features where identified: classes\textit{A, B} and \textit{C}. Class \textit{A} features being features whose data can be obtained directly in the user profile, while class \textit{B} features require a simple computation, and \textit{C} features require heavier computations.\\

We implemented the functions and used them to extract 2 features set out of the data: a \textit{class A} features set and a class C features set, which encompasses the features of every classes (\textit{A, B} and \textit{C}).

\subsection{Data labeling}
For each initial dataset (E13, FAK, FSF, HUM, INT, TFP, TWT) the label of the users is known. The TFP and HUM dataset only contain real users, while the other dataset contain fake users.\\

Therefor, when generating the class A and class C features set, we add the label 'human' for the features set based on TFP and HUM, and the label 'bot' to the other. These labels will actually be numbers in the features set : 1 for bots, and 0 for humans.

\section{Process Optimization}
The amount of data that we had to manipulate by no means huge or overwhelming, but it was still big enough so that, if not careful, some optimization issues could arise.\\

Developing using Python, we used the popular pandas library and its DataFrame object to manipulate our data and perform computations. Soon however, the program was plagued by an abnormally slow execution time.\\

The issue came from manually looping on DataFrame object, either by user a \textit{for} loop, or by using the DataFrame.iterrows() function. This led to processing time that could reach between 10 and 20 seconds per user record. Considering that we had several thousands users, another solution had to be found.\\\\
This part of the project was definetly the most time consuming and involved a lot of small tricks to achieve the expected result for certain features.\\
As explained above, after a first implementation of certain features we realized that the extraction was extremly slow, we then decided to cut down the testing set to 10 users to ensure a functional extractor without waiting for 30 min after each execution. This already improved our testing time.\\
After that we realized that for the B class features we were getting very long execution times. Because the B class features are realted to tweets, depending on the users this amount can be in the thousands and for some features this involved looping a lot.

\subsection{DataFrame.apply() and lambda functions}
We found out that there was several efficient ways to loop over a DataFrame. The easiest way being through the use of lambda functions which will automatically be executed on every row of the Dataframe, without requiring a manual management of the iterative process.\\

The result was efficient, and the processing time per user came down from 10-20s to about 0.3s. We decided that this gain in performance was good but with 3900 users to process we're still talking of 1300s which is a really long processing time. To improve that we researched optimisation techniques with Pandas.SerieS. And using built in functions and conditions on the Series we kept improving our processing time. For some of our features we obtained huge improvements alowing to reduce most of the B features to 0.05s (except for the HUM dataset because of the high amount of data to check).\\\\

Because the built in functions are limited and Series conditional selection is limited for some of the features we add to be creative. I have some example I want to showcase:

\begin{verbatim}
def is_favorite(userRow,tweetsDF):
	tweets = cache.get_user_tweets(int(userRow['id']),tweetsDF)
	fav = tweets['favorite_count'] != 0
	return int(not tweets[fav].empty)
\end{verbatim}
Here we recover a dataframe with only the tweets of the user. We create a conditional variable that selects everthing different then 0. If we apply it to that dataframe we are going to get a Series with True and False values, by checking for tweets[fav].empty we are looking if at least one of the values isn't False.
\\ A last example: 
\begin{verbatim}
def uses_foursquare(userRow,tweetsDF):
	all_tweets = cache.get_user_tweets(int(userRow['id']),tweetsDF)
	res = "foursquare" in all_tweets['source'].str.cat()
\end{verbatim}
Here instead of looping through the tweet sources and adding them to a string to finally check if the searched string is in it we adopt another approach: isolate the Series of the dataframe and the concatenate the strings using the builtin function. It achieves the same thing but reduces the processing time drasticaly.\\\\

There are many other features that have been approached in unconventional ways to achieve better results. \\

Unfortunately we found out too late of a method called Vectorization which would arguably perform even faster. But this is not a problem since we look forward in the future to try it out and achieve the most optimized result possible.

\section{Classifiers}
In \textit{Cresci et al.} the following 8 classifiers are proposed: Random Forest, Decorate, Decision Tree, Adaptive Boosting, Bayesian Network, k-Nearest Neighbors, Logistic Regression and Support Vector Machine.\\

For the sake of simplicity, we decided to focus on the following classifiers:Random Forest,  Decision Tree, Adaptive Boosting, k-Nearest Neighbors, Logistic Regression and Support Vector Machine.\\

\subsection{Results}
After training our classifierson our generated datasets, we obtain the following resutls:

\begin{tabular}{lcccccc}
	\hline
	\textbf{Algorithm} 	& \textbf{Accuracy} 	& \textbf{Precision} & \textbf{Recall} &\textbf{F-M} 	& \textbf{MCC} & AUC\\
	\hline
	\textit{Class A}\\
	
	J48	&	1		&	1		&	1		& 1 	&	1	& 1   \\
	LR	&	0.997	&	0.997	&	0.997	& 0.997 &	0.994 & 0.997  \\
	AB	&	1		&	1		&	1		& 1 	&	1	& 1   \\	
	SVM	&	0.678	&	0.999	&	0.356	& 0.525 & 0.464 & 0.78\\
	RF	&	0.999	&	1		&	0.999	& 0.999	&	0.999	& 0.999 \\
	kNN	&	0.944	&	0.937	&	0.951	& 0.944 & 0.888 & 0.944\\
	
	\hline
\end{tabular}
	
	\paragraph{Accuracy}
	Accuracy measure the percentage of samples correctly identified in both classe (human, bot). In our case, the accuracy seems very high, except for Support Vector Machines, for a reason that we do not explain.
	
	\paragraph{Precision}
	A high precision indicates that the samples that were classified as bots actually are. Here, the precision seems very high for every classifier. The high precision of the SVM combined with its low accuracy indicates that when the classifier labels a sample as bot, it is very confident and is almost always right. But the classifier stays silent on many samples that are bot and fails to identify them as such.
	
	\paragraph{Recall}
	The recall identifies the relevant samples that have been missed by the classifier. As excpected, based on the accuracy and precision, the SVM classifier score low, meaning that when it sees a bot, it often doesn't recognizes it as such. The rest of the classifier perform very well.
	
	
	\paragraph{F-M}
	The F-Measure gives a global assessment of the quality of the classifier. It is high for all classifiers, except for the SVM.
	
	\paragraph{MCC}
	Similar the the F-M, the MCC tries to give a single value that represent the quality of the classifier. All classifiers score well except for the SVM.	
	\paragraph{AUC}
	
	As you can see, the classifiers seem to 
	Due to an execution error caused by a bad value, the results for class C couldn't be displayed
	
\section{Conclusion}
In this project, we first familiarized ourselves with the various dataset provided before starting building features that emulated the 5 studies described in Cresci \textit{et al}. \\
Based on those features and and the Class A, and Class C feature set provided by Cresci \textit{et al.}, we generated for each of the 6 datasets, a pair of Class A and Class C features dataset for which every row was labeled as either 'human' or 'bot'.\\
Once these features were generated and labeled, they were used them to train various \textit{classifiers} in hope to be able to distinguish fake users from legitimate users.

\bibliography{bibliography}
\bibliographystyle{plain}
\end{document}